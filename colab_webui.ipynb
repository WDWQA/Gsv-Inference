{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/X-T-E-R/GPT-SoVITS-Inference/blob/stable/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o6a8GS2lWQM"
      },
      "source": [
        "环境配置 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9b7iFV3dm1f"
      },
      "outputs": [],
      "source": [
        "%pip install -q condacolab\n",
        "%pip install ipykernel\n",
        "\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/X-T-E-R/GPT-SoVITS-Inference\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS-Inference\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NgxXg5sjv7z"
      },
      "outputs": [],
      "source": [
        "# @title Download pretrained models 下载预训练模型\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "\n",
        "%cd /content/GPT-SoVITS-Inference/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS-Inference/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS-Inference/GPT_SoVITS/pretrained_models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "设置 settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# settings\n",
        "%cd /content/GPT-SoVITS-Inference\n",
        "import json\n",
        "config = {}\n",
        "try:\n",
        "    with open(\"Inference/config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        config = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"config.json not found, please check the file path.\")\n",
        "\n",
        "config[\"locale\"] = \"en-US\"\n",
        "config[\"is_share\"] = \"true\"\n",
        "config[\"models_path\"] = \"trained\"\n",
        "\n",
        "try:\n",
        "    with open(\"Inference/config.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(config, f, ensure_ascii=False, indent=4)\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"config.json not found, please check the file path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Collecting rarfile\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/62/fc/ab37559419ca36dd8dd317c3a98395ed4dcee2beeb28bf6059b972906727/rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: rarfile\n",
            "Successfully installed rarfile-4.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "character_name: 银狼\n"
          ]
        },
        {
          "ename": "NotRarFile",
          "evalue": "Not a RAR file",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotRarFile\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m rarfile\u001b[38;5;241m.\u001b[39mPATH_SEP \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m rarfile\u001b[38;5;241m.\u001b[39mDEFAULT_ENCODING \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbk\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mrarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRarFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m rf:\n\u001b[0;32m     43\u001b[0m     rf\u001b[38;5;241m.\u001b[39mextractall(path\u001b[38;5;241m=\u001b[39moutput_path)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# os.remove(output_path + \"file.zip\")\u001b[39;00m\n",
            "File \u001b[1;32me:\\AItools\\GPT-SoVITS-Inference\\runtime\\lib\\site-packages\\rarfile.py:711\u001b[0m, in \u001b[0;36mRarFile.__init__\u001b[1;34m(self, file, mode, charset, info_callback, crc_check, errors, part_only)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRarFile supports only mode=r\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 711\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\AItools\\GPT-SoVITS-Inference\\runtime\\lib\\site-packages\\rarfile.py:930\u001b[0m, in \u001b[0;36mRarFile._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_parser \u001b[38;5;241m=\u001b[39m p5  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotRarFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot a RAR file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_parser\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_parser\u001b[38;5;241m.\u001b[39mcomment\n",
            "\u001b[1;31mNotRarFile\u001b[0m: Not a RAR file"
          ]
        }
      ],
      "source": [
        "%cd /content/GPT-SoVITS-Inference\n",
        "#@title Import model 导入模型 (HuggingFace)\n",
        "\n",
        "# put your character folder in trained/\n",
        "# or use the code below to download the HuggingFace model\n",
        "# please check if the zip contains the model files and a audio_file named by its content\n",
        "# if not, please modify the folder manually in trained/ \n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import urllib.parse\n",
        "\n",
        "\n",
        "# special thanks to baicai1145 for providing the model(and its copyright belongs to the Mihoyo)\n",
        "# copy the link from the download button on the model page\n",
        "hf_link = 'https://huggingface.co/baicai1145/GPT-SoVITS-STAR/resolve/main/%E9%93%B6%E7%8B%BC.zip?download=true' #@param {type: \"string\"}\n",
        "\n",
        "# get the name of the character folder, or you can set it manually\n",
        "character_name = urllib.parse.unquote(os.path.basename(hf_link).rsplit('.', 1)[0])\n",
        "\n",
        "print(f'character_name: {character_name}')\n",
        "\n",
        "output_path = os.path.join('trained', character_name)\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "zip_file_path = os.path.join(output_path, 'file.zip')\n",
        "\n",
        "# download the zip file\n",
        "response = requests.get(hf_link)\n",
        "with open(zip_file_path, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# extract the zip file\n",
        "import zipfile\n",
        "\n",
        "def get_decoder(file_name: str):\n",
        "    try:\n",
        "        return file_name.encode('cp437').decode('gbk')\n",
        "    except:\n",
        "        return file_name\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    for file_info in zip_ref.infolist():\n",
        "        # 解码并重新编码文件名\n",
        "        encoded_file_name = get_decoder(file_info.filename)\n",
        "        new_path = os.path.join(output_path, encoded_file_name)\n",
        "        source = zip_ref.open(file_info.filename)\n",
        "        # 判断f是否是目录，目录的结尾是'/'或'\\'\n",
        "        if encoded_file_name[-1] not in ['\\\\','/']:\n",
        "            with open(new_path,'wb') as file:\n",
        "                file.write(zip_ref.read(file_info.filename))\n",
        "                file.close()\n",
        "        else:\n",
        "            os.makedirs(new_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oRGUzkrk8C7"
      },
      "outputs": [],
      "source": [
        "# @title launch WebUI 启动WebUI\n",
        "# special thanks to @rc4 \n",
        "%pip install ipykernel\n",
        "%cd /content/GPT-SoVITS-Inference/\n",
        "!/usr/local/bin/python  app.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
